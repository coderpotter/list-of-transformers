Here is a list of transformers on [HuggingFace](https://huggingface.co/transformers/index.html) for easy reference. This list only has masked and causal language models.

## Glossary
* Model Name: The name of the model and a link to its HuggingFace page
* Published: The venue where the paper releasing the model was published and a link to the paper
* Research Group: The company/institution that released the model
* Language Modeling: "M" or "C" for "Masked" or "Causal"
* Variant: The size of the model (typically base, large, etc.)
* `vocab_size`: Number of tokens in the vocabulary
* `embedding_size`: Size of vocabulary embeddings
* `num_hidden_layers`: Number of hidden layers in the encoder
* `num_attention_heads`: Number of attention heads for each attention layer in the encoder
* Params: Number of _learnable_ elements in the transformer


## List of Transformers

Model Name | Published | Research Group | Variant | `vocab_size` | `embedding_size` | `num_hidden_layers` | `num_attention_heads` | Params
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| [ALBERT](https://huggingface.co/transformers/model_doc/albert.html) | [ICLR 2020](https://iclr.cc/virtual_2020/poster_H1eA7AEtvS.html)
| [BART](https://huggingface.co/transformers/model_doc/bart.html) | [ACL 2020](https://aclanthology.org/2020.acl-main.703.pdf)
